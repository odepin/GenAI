{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6270f25c",
   "metadata": {},
   "source": [
    "# LLM Project To Build and Fine Tune a Large Language Model\n",
    "\n",
    "The project uses Retrieval Augmented Generation (RAG) with OpenAI to build a knowledge-based online shopping chatbot. RAG reduces hallucinations and delivers reliable answers by integrating external information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437c973",
   "metadata": {},
   "source": [
    "## Package Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c1620-b5bd-43d9-988a-b4775c66ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warning messages\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import PyTorch for deep learning, Evaluate for model evaluation metrics, and Time for performance timing.\n",
    "import torch\n",
    "import evaluate\n",
    "import time\n",
    "\n",
    "# Import Pandas for data manipulation and NumPy for numerical operations.\n",
    "import pandas as pd\n",
    "\n",
    "# Import load_dataset from Datasets library for loading and managing datasets.\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import various classes from Transformers library for NLP model loading, tokenization, configuration, training, and random seed setting.\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# Import PEFT (Parameter-Efficient Fine-Tuning) classes for advanced model configuration and adaptation.\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6aed0-085f-4d58-a8e9-1d1b00ae9b91",
   "metadata": {},
   "source": [
    "## Model Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75512899-12e6-43fd-98f4-9c7d09847510",
   "metadata": {},
   "source": [
    "### Load Dataset and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc43cb5-7ae6-4921-936a-f6962052d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device for PyTorch operations\n",
    "DEVICE = \"cpu\"\n",
    "torch_device = torch.device(DEVICE)\n",
    "\n",
    "# Load the dataset from Hugging Face's datasets library\n",
    "hugging_face_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(hugging_face_dataset_name)\n",
    "\n",
    "# Load the original FLAN-T5 model and tokenizer from Hugging Face's Transformers library\n",
    "model_name = \"google/flan-t5-base\"\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(torch_device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to calculate the number of trainable and total model parameters, and their percentage.\n",
    "def number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0  # Initialize count of trainable parameters\n",
    "    all_model_params = 0  # Initialize count of all parameters\n",
    "    for _, param in model.named_parameters():  # Iterate through all model parameters\n",
    "        all_model_params += param.numel()  # Count total parameters\n",
    "        if param.requires_grad:  # Check if parameter is trainable\n",
    "            trainable_model_params += param.numel()  # Count trainable parameters\n",
    "\n",
    "    # Calculate and format the result\n",
    "    result = f\"trainable model parameters: {trainable_model_params}\\n\"\n",
    "    result += f\"all model parameters: {all_model_params}\\n\"\n",
    "    result += f\"Percentage of model params: {(trainable_model_params / all_model_params) * 100}\"\n",
    "    return result\n",
    "\n",
    "# Calculates and displays the count of trainable parameters, the count of all parameters,\n",
    "# and the percentage of trainable parameters relative to all parameters in the model.\n",
    "print(number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad2c79-e6ce-49ad-856a-588f61635429",
   "metadata": {},
   "source": [
    "### Test the Model With Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fefc6a-53a9-4500-b598-a9ca00a3c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the dialogue to be summarized\n",
    "index = 200\n",
    "\n",
    "# Extract the dialogue and summary from the dataset\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "# Construct the prompt for summarizing the conversation\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a summary using the original model\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=200,\n",
    "    )[\n",
    "        0\n",
    "    ],  # Generate a summary\n",
    "    skip_special_tokens=True,  # Skip special tokens when decoding\n",
    ")\n",
    "\n",
    "# Print the prompt, baseline human summary, and model-generated summary\n",
    "dash_line = \"-\".join(\"\" for x in range(100))  # Create a dashed line\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")  # Print the input prompt\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary:\\n{summary}\\n\")  # Print the baseline human summary\n",
    "print(dash_line)\n",
    "print(f\"Model Generation - Zero Shot: \\n{output}\")  # Print the model-generated summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22088e95-6104-486c-be81-81194e1fded1",
   "metadata": {},
   "source": [
    "### Perform Full Fine-Tuning\n",
    "\n",
    "#### Preprocess the Dialog-Summary Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd762da7-873a-452a-ae18-de765b32a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function to prepare examples for sequence-to-sequence models.\n",
    "def tokenize_function(example):\n",
    "    start_prompt = \"Summarize the following conversation.\\n\\n\"  # Starting prompt for dialogue summarization\n",
    "    end_prompt = \"\\n\\nSummary: \"  # Ending prompt for dialogue summarization\n",
    "    # Construct prompts for each dialogue in the example\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    # Tokenize prompts and summaries\n",
    "    example[\"input_ids\"] = tokenizer(\n",
    "        prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    example[\"labels\"] = tokenizer(\n",
    "        example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    return example\n",
    "\n",
    "# Tokenize the dataset using the tokenize_function, batched for efficiency\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns from the tokenized dataset\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"id\", \"topic\", \"dialogue\", \"summary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36a547-96bf-4bfc-8f4b-bbeed5096e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the tokenized dataset to keep examples with indices divisible by 100, retaining indices\n",
    "tokenized_datasets = tokenized_datasets.filter(\n",
    "    lambda example, index: index % 100 == 0, with_indices=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad14994-44d2-4be0-bbd1-4930cf8b0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the datasets\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "# Print the tokenized datasets\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e16f70-8638-4e8b-971c-16a09ad9dd13",
   "metadata": {},
   "source": [
    "#### Fine-Tune the Model With the Preprocessed Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e9712-d0a5-43a4-b041-0284f44f057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory for saving training checkpoints and logs\n",
    "output_dir = f\"../models/dialogue-summary-training-{str(int(time.time()))}\"\n",
    "\n",
    "# Define the training arguments including output directory, learning rate, number of epochs, etc.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1,\n",
    "    # Uncomment to use CPU\n",
    "    bf16=True,\n",
    "    use_cpu=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer object for training the model\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],  # Validation dataset\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer object\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebf956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the instructed model from a pre-trained checkpoint and move it to the specified device\n",
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"../models/full/\").to(\n",
    "    torch_device\n",
    ")\n",
    "\n",
    "# Move the original model to the specified device\n",
    "original_model = original_model.to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c322fc2-062c-443d-a8b1-eddd3ca7741a",
   "metadata": {},
   "source": [
    "### Evaluate the Model Qualitatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51766726-3189-4b08-82cc-ab9ccab2ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the dialogue to be summarized\n",
    "index = 200\n",
    "\n",
    "# Extract the dialogue and human baseline summary from the dataset\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "human_baseline_summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "# Construct the prompt for summarizing the conversation\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate summaries using the original and instructed models\n",
    "original_outputs = original_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1),\n",
    ")\n",
    "original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "instruct_outputs = instruct_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1),\n",
    ")\n",
    "instruct_text_output = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the input prompt, baseline human summary, and model-generated summaries\n",
    "dash_line = \"-\".join(\"\" for x in range(100))  # Create a dashed line\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")  # Print the input prompt\n",
    "print(dash_line)\n",
    "print(\n",
    "    f\"Baseline Human Summary:\\n{human_baseline_summary}\\n\"\n",
    ")  # Print the baseline human summary\n",
    "print(dash_line)\n",
    "print(\n",
    "    f\"Original Model Generation - Zero Shot: \\n{original_text_output}\"\n",
    ")  # Print the original model-generated summary\n",
    "print(dash_line)\n",
    "print(\n",
    "    f\"Instruct Model Generation - Fine Tune: \\n{instruct_text_output}\"\n",
    ")  # Print the instructed model-generated summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce6bb6-0889-4fe2-9771-a5fc50560215",
   "metadata": {},
   "source": [
    "### Evaluate the Model Quantitatively (With Rouge Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be96e9ae-014c-4e89-b80e-1c97aa78ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ROUGE evaluator\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a8655-c9cd-4c12-ae09-81ad9532bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dialogues from the dataset\n",
    "dialogue = dataset[\"test\"][0:10][\"dialogue\"]\n",
    "\n",
    "# Extract the human baseline summaries from the dataset\n",
    "human_baseline_summaries = dataset[\"test\"][0:10][\"summary\"]\n",
    "\n",
    "# Initialize lists to store summaries generated by the original and instructed models\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "# Generate summaries for each dialogue using both the original and instructed models\n",
    "for _, dialogue in enumerate(dialogue):\n",
    "    # Construct the prompt for summarizing the conversation\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Generate summaries using the original model\n",
    "    original_outputs = original_model.generate(\n",
    "        input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200)\n",
    "    )\n",
    "    original_text_output = tokenizer.decode(\n",
    "        original_outputs[0], skip_special_tokens=True\n",
    "    )\n",
    "    original_model_summaries.append(original_text_output)\n",
    "\n",
    "    # Generate summaries using the instructed model\n",
    "    instruct_outputs = instruct_model.generate(\n",
    "        input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200)\n",
    "    )\n",
    "    instruct_text_output = tokenizer.decode(\n",
    "        instruct_outputs[0], skip_special_tokens=True\n",
    "    )\n",
    "    instruct_model_summaries.append(instruct_text_output)\n",
    "\n",
    "# Create a DataFrame to store human baseline, original model, and instructed model summaries\n",
    "zipped_summaries = list(\n",
    "    zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries)\n",
    ")\n",
    "df = pd.DataFrame(zipped_summaries, columns=[\"human\", \"original\", \"instruct\"])\n",
    "\n",
    "# Display the dataframe.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55838ad-c8bc-4e05-a94a-ddff480da233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROUGE scores for summaries generated by the original model compared to human baseline summaries\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,  # Predicted summaries generated by the original model\n",
    "    references=human_baseline_summaries,  # Reference (human baseline) summaries\n",
    "    use_aggregator=True,  # Use aggregator for multiple references\n",
    "    use_stemmer=True,  # Use stemmer to preprocess text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d075dc6-4c89-4ea4-b8bb-0b81e5fe9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROUGE scores for summaries generated by the instructed model compared to corresponding human baseline summaries\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,  # Predicted summaries generated by the instructed model\n",
    "    references=human_baseline_summaries[\n",
    "        0 : len(instruct_model_summaries)\n",
    "    ],  # Corresponding human baseline summaries\n",
    "    use_aggregator=True,  # Use aggregator for multiple references\n",
    "    use_stemmer=True,  # Use stemmer to preprocess text\n",
    ")\n",
    "\n",
    "# Print the ROUGE results for both original and instructed models\n",
    "print(\n",
    "    f\"Original Model: \\n{original_model_results}\"\n",
    ")  # Print ROUGE results for the original model\n",
    "print(\n",
    "    f\"Instruct Model: \\n{instruct_model_results}\"\n",
    ")  # Print ROUGE results for the instructed model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13167f8b-86c3-4c2f-b7b6-b615b78178c7",
   "metadata": {},
   "source": [
    "## Parameter Efficient Fine-Tuning With LoRA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8166e-6506-4ad8-a1b0-b1256965a65e",
   "metadata": {},
   "source": [
    "### Setup the PEFT/LoRA Model for Fine-Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba24b4e-5bba-465e-90c0-054bea5a24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lora configuration for a sequence-to-sequence LM task\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  # Number of heads in the Lora module\n",
    "    lora_alpha=32,  # Alpha parameter for Lora\n",
    "    target_modules=[\"q\", \"v\"],  # List of target modules for Lora attention\n",
    "    lora_dropout=0.05,  # Dropout probability for Lora\n",
    "    bias=\"none\",  # Bias type for Lora\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  # Task type for the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1122785-3957-41a9-b1d9-844d7d7da279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the PEFT model by integrating the original model with the specified Lora configuration\n",
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "\n",
    "# Print the number of trainable parameters in the PEFT model\n",
    "print(number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35033e6-9d34-4874-8f30-da488b5001ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory for saving training checkpoints and logs\n",
    "output_dir = f\"../models/peft-dialogue-summary-training-{str(int(time.time()))}\"\n",
    "\n",
    "# Define the training arguments including output directory, learning rate, number of epochs, etc.\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,  # Automatically find the batch size\n",
    "    output_dir=output_dir,  # Output directory for saving checkpoints and logs\n",
    "    learning_rate=1e-3,  # Learning rate for training\n",
    "    num_train_epochs=100,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    logging_steps=1,  # Log training metrics every specified number of steps\n",
    "    max_steps=1,  # Maximum number of training steps\n",
    "    # Uncomment to use CPU\n",
    "    bf16=True,\n",
    "    use_cpu=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer object for training the PEFT model\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,  # PEFT model to be trained\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],  # Validation dataset\n",
    ")\n",
    "\n",
    "# Train the PEFT model using the Trainer object\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c74cb-571b-46be-affc-13c4f995309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model for PEFT fine-tuning\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer for the base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Load the PEFT model from the saved checkpoint\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    peft_model_base,  # Base model for PEFT fine-tuning\n",
    "    \"../models/peft/\",  # Directory containing the PEFT fine-tuned model checkpoint\n",
    ").to(\n",
    "    torch_device\n",
    ")  # Move the model to the specified device\n",
    "\n",
    "# Move the original model to the specified device\n",
    "original_model = original_model.to(torch_device)\n",
    "\n",
    "# Define the index of the dialogue to summarize\n",
    "index = 200\n",
    "\n",
    "# Extract the dialogue and human baseline summary from the dataset\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "human_baseline_summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "# Construct the prompt for summarizing the conversation\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate a summary using the original model\n",
    "original_outputs = original_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1),\n",
    ")\n",
    "original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate a summary using the PEFT fine-tuned model\n",
    "peft_outputs = peft_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1),\n",
    ")\n",
    "peft_text_output = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the dialogue, human baseline summary, and generated summaries\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary:\\n{human_baseline_summary}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"Original Model Generation - Zero Shot: \\n{original_text_output}\")\n",
    "print(dash_line)\n",
    "print(f\"PEFT Model Generation - Zero Shot: \\n{peft_text_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the dialogues and human baseline summaries from the dataset\n",
    "dialogue = dataset[\"test\"][0:10][\"dialogue\"]\n",
    "human_baseline_summaries = dataset[\"test\"][0:10][\"summary\"]\n",
    "\n",
    "# Initialize lists to store the generated summaries from both original and PEFT models\n",
    "original_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "# Iterate through each dialogue and generate summaries using both models\n",
    "for _, dialogue_text in enumerate(dialogue):\n",
    "    # Construct the prompt for summarizing the conversation\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue_text}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Generate a summary using the PEFT fine-tuned model\n",
    "    peft_outputs = peft_model.generate(\n",
    "        input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200)\n",
    "    )\n",
    "    peft_text_output = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_summaries.append(peft_text_output)\n",
    "\n",
    "# Combine the human baseline summaries with the generated summaries into a DataFrame\n",
    "zipped_summaries = list(\n",
    "    zip(human_baseline_summaries, original_model_summaries, peft_model_summaries)\n",
    ")\n",
    "df = pd.DataFrame(zipped_summaries, columns=[\"human\", \"original\", \"peft\"])\n",
    "\n",
    "# Compute ROUGE scores for the summaries generated by the PEFT model\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0 : len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# Print the ROUGE scores for each model\n",
    "print(f\"Original Model Results: \\n{original_model_results}\")\n",
    "print(f\"Instruct Model Results: \\n{instruct_model_results}\")\n",
    "print(f\"PEFT Model Results: \\n{peft_model_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".finetuningvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
