{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a480f6f3",
   "metadata": {},
   "source": [
    "# RAG Ecosystem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b98a0",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Understanding Basic RAG System](#part1)\n",
    "  - [Indexing Phase](#part1-1)\n",
    "  - [Retrieval](#part1-2)\n",
    "  - [Generation](#part1-3)\n",
    "- [Advanced Query Transformations](#part2)\n",
    "  - [Multi-Query Generation](#part2-1)\n",
    "  - [RAG-Fusion](#part2-2)\n",
    "  - [Decomposition](#part2-3)\n",
    "  - [Step-Back Prompting](#part2-4)\n",
    "  - [HyDE](#part2-5)\n",
    "- [Routing & Query Construction](#part3)\n",
    "  - [Logical Routing](#part3-1)\n",
    "  - [Semantic Routing](#part3-2)\n",
    "  - [Query Structuring](#part3-3)\n",
    "- [Advanced Indexing Strategies](#part4)\n",
    "  - [Multi-Representation Indexing](#part4-1)\n",
    "  - [Hierarchical Indexing (RAPTOR) Knowledge Tree](#part4-2)\n",
    "  - [Token-Level Precision (ColBERT)](#part4-3)\n",
    "- [Advanced Retrieval & Generation](#part5)\n",
    "  - [Dedicated Re-ranking](#part5-1)\n",
    "  - [Self-Correction using AI Agents](#part5-2)\n",
    "  - [Impact of Long Context](#part5-3)\n",
    "- [Manual RAG Evaluation](#part6)\n",
    "  - [The Core Metrics: What Should We Measure?](#part6-1)\n",
    "  - [Building Evaluators from Scratch with LangChain](#part6-2)\n",
    "- [Evaluation with Frameworks](#part7)\n",
    "  - [Rapid Evaluation with deepeval](#part7-1)\n",
    "  - [Another Powerful Alternative with grouse](#part7-2)\n",
    "  - [Evaluation with RAGAS](#part7-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5610337",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "# Basic RAG System\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517d6a4",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_all",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "import uuid\n",
    "import datetime\n",
    "import requests\n",
    "\n",
    "from typing import Literal, Optional\n",
    "from operator import itemgetter\n",
    "\n",
    "# torch import\n",
    "import torch\n",
    "\n",
    "# LangChain imports\n",
    "from langchain import hub\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.embeddings import FastEmbedEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# LangChain Community imports\n",
    "from langchain_community.document_loaders import WebBaseLoader, YoutubeLoader\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# LangChain Core imports\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangChain OpenAI imports\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Evaluation framework imports\n",
    "from deepeval.metrics import GEval, FaithfulnessMetric, ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "from grouse import EvaluationSample, GroundedQAEvaluator\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    ")\n",
    "\n",
    "# RAGatouille import\n",
    "from fastembed import TextEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set LangChain API endpoint and API key for tracing with LangSmith\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\") \n",
    "\n",
    "# Set OpenAI API key for using OpenAI models\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set Cohere API key for using Cohere\n",
    "os.environ['COHERE_API_KEY'] = os.getenv(\"COHERE_API_KEY\") \n",
    "\n",
    "# Set User Agent\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "\n",
    "# Uncomment to use CPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100be33",
   "metadata": {},
   "source": [
    "<a id='part1-1'></a>\n",
    "## Indexing Phase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbf0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a web document loader with specific parsing instructions\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),  # URL of the blog post to load\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")  # Only parse specified HTML classes\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Load the filtered content from the web page into documents\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter to divide text into chunks of 1000 characters with 200-character overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the loaded documents into smaller chunks\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e824e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the text chunks and store them in a Chroma vector store for similarity search\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=OpenAIEmbeddings()  # Use OpenAI's embedding model to convert text into vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b59828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant documents for a query\n",
    "docs = retriever.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "# Print the content of the first retrieved document\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42d28c",
   "metadata": {},
   "source": [
    "<a id='part1-3'></a>\n",
    "## Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f72b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull a pre-made RAG prompt from LangChain Hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# printing the prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the full RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511eb59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question using the RAG chain\n",
    "response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1833de",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "# Advanced Query Transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2de569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the blog post\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index the chunks in a Chroma vector store\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Create our retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa37693",
   "metadata": {},
   "source": [
    "<a id='part2-1'></a>\n",
    "## Multi-Query Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57498a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for generating multiple queries\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain to generate the queries\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fc1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generated_queries_list = generate_queries.invoke({\"question\": question})\n",
    "\n",
    "# Print the generated queries\n",
    "for i, q in enumerate(generated_queries_list):\n",
    "    print(f\"{i+1}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" A simple function to get the unique union of retrieved documents \"\"\"\n",
    "    # Flatten the list of lists and convert each Document to a string for uniqueness\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Build the retrieval chain\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "# Invoke the chain and check the number of documents retrieved\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "print(f\"Total unique documents retrieved: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final RAG chain\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabfa157",
   "metadata": {},
   "source": [
    "<a id='part2-2'></a>\n",
    "## RAG-Fusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5475a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal Rank Fusion that intelligently combines multiple ranked lists \"\"\"\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # The core of RRF: documents ranked higher (lower rank value) get a larger score\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort documents by their new fused scores in descending order\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a slightly different prompt for RAG-Fusion\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Build the new retrieval chain with RRF\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Total re-ranked documents retrieved: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0aab1f",
   "metadata": {},
   "source": [
    "<a id='part2-3'></a>\n",
    "## Decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition prompt\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain to generate sub-questions\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Generate and print the sub-questions\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "sub_questions = generate_queries_decomposition.invoke({\"question\": question})\n",
    "print(sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3284577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# A list to hold the answers to our sub-questions\n",
    "rag_results = []\n",
    "for sub_question in sub_questions:\n",
    "    # Retrieve documents for each sub-question\n",
    "    retrieved_docs = retriever.invoke(sub_question)\n",
    "    \n",
    "    # Use our standard RAG chain to answer the sub-question\n",
    "    answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "    rag_results.append(answer)\n",
    "\n",
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# Format the Q&A pairs into a single context string\n",
    "context = format_qa_pairs(sub_questions, rag_results)\n",
    "\n",
    "# Final synthesis prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the original question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"context\": context, \"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1825be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot examples to teach the model how to generate step-back (more generic) questions\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel's was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel's personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Define how each example is formatted in the prompt\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),  # User input\n",
    "    (\"ai\", \"{output}\")     # Model's response\n",
    "])\n",
    "\n",
    "# Wrap the few-shot examples into a reusable prompt template\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Full prompt includes system instruction, few-shot examples, and the user question\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are an expert at world knowledge. Your task is to step back and paraphrase a question \"\n",
    "     \"to a more generic step-back question, which is easier to answer. Here are a few examples:\"),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{question}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b3610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chain to generate step-back questions using the prompt and an OpenAI model\n",
    "generate_queries_step_back = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "\n",
    "# Run the chain on a specific question\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "# Output the original and generated step-back question\n",
    "print(f\"Original Question: {question}\")\n",
    "print(f\"Step-Back Question: {step_back_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for the final response\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "# The full chain\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the original question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83694b96",
   "metadata": {},
   "source": [
    "<a id='part2-5'></a>\n",
    "## HyDE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE prompt\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain to generate the hypothetical document\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Generate and print the hypothetical document\n",
    "hypothetical_document = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "print(hypothetical_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve documents using the HyDE approach\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "# Use our standard RAG chain to generate the final answer from the retrieved context\n",
    "response = final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237d2ab",
   "metadata": {},
   "source": [
    "<a id='part3'></a>\n",
    "# Routing & Query Construction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e24fb2",
   "metadata": {},
   "source": [
    "<a id='part3-1'></a>\n",
    "## Logical Routing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data model for our router's output\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"A data model to route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    # The 'datasource' field must be one of the three specified literal strings.\n",
    "    # This enforces a strict set of choices for the LLM.\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,  # The '...' indicates that this field is required.\n",
    "        description=\"Given a user question, choose which datasource would be most relevant for answering their question.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b663642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "# Create a new LLM instance that is \"structured\" to output our Pydantic model\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# The system prompt provides the core instruction for the LLM's task.\n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "# The full prompt template combines the system message and the user's question.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the complete router chain\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "# Invoke the router and check the result\n",
    "result = router.invoke({\"question\": question})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5110a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    \"\"\"A function to determine the downstream logic based on the router's output.\"\"\"\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        # In a real app, this would be a complete RAG chain for Python docs\n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        # This would be the chain for JavaScript docs\n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        # And this for Go docs\n",
    "        return \"chain for golang_docs\"\n",
    "\n",
    "# The full chain now includes the routing and branching logic\n",
    "full_chain = router | RunnableLambda(choose_route)\n",
    "\n",
    "# Let's run the full chain\n",
    "final_destination = full_chain.invoke({\"question\": question})\n",
    "\n",
    "print(final_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e018b4",
   "metadata": {},
   "source": [
    "<a id='part3-2'></a>\n",
    "## Semantic Routing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A prompt for a physics expert\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# A prompt for a math expert\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Store our templates and their embeddings for comparison\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "def prompt_router(input):\n",
    "    \"\"\"A function to route the input query to the most similar prompt template.\"\"\"\n",
    "    # 1. Embed the incoming user query\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    \n",
    "    # 2. Compute the cosine similarity between the query and all prompt templates\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    \n",
    "    # 3. Find the index of the most similar prompt\n",
    "    most_similar_index = similarity.argmax()\n",
    "    \n",
    "    # 4. Select the most similar prompt template\n",
    "    chosen_prompt = prompt_templates[most_similar_index]\n",
    "    \n",
    "    print(f\"DEBUG: Using {'MATH' if most_similar_index == 1 else 'PHYSICS'} template.\")\n",
    "    \n",
    "    # 5. Return the chosen prompt object\n",
    "    return PromptTemplate.from_template(chosen_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final chain that combines the router with the LLM\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)  # Dynamically select the prompt\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a physics question\n",
    "print(chain.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d018d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a YouTube transcript to inspect its metadata\n",
    "# docs = YoutubeLoader.from_youtube_url(\n",
    "#    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\", add_video_info=True\n",
    "#).load()\n",
    "\n",
    "# Print the metadata of the first document\n",
    "# print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed382555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"A data model for searching over a database of tutorial videos.\"\"\"\n",
    "\n",
    "    # The main query for a similarity search over the video's transcript.\n",
    "    content_search: str = Field(..., description=\"Similarity search query applied to video transcripts.\")\n",
    "    \n",
    "    # A more succinct query for searching just the video's title.\n",
    "    title_search: str = Field(..., description=\"Alternate version of the content search query to apply to video titles.\")\n",
    "    \n",
    "    # Optional metadata filters\n",
    "    min_view_count: Optional[int] = Field(None, description=\"Minimum view count filter, inclusive.\")\n",
    "    max_view_count: Optional[int] = Field(None, description=\"Maximum view count filter, exclusive.\")\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(None, description=\"Earliest publish date filter, inclusive.\")\n",
    "    latest_publish_date: Optional[datetime.date] = Field(None, description=\"Latest publish date filter, exclusive.\")\n",
    "    min_length_sec: Optional[int] = Field(None, description=\"Minimum video length in seconds, inclusive.\")\n",
    "    max_length_sec: Optional[int] = Field(None, description=\"Maximum video length in seconds, exclusive.\")\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        \"\"\"A helper function to print the populated fields of the model.\"\"\"\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None:\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36695efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for the query analyzer\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{question}\")])\n",
    "structured_llm = llm.with_structured_output(TutorialSearch)\n",
    "\n",
    "# The final query analyzer chain\n",
    "query_analyzer = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48323354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: A simple query\n",
    "query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea4690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: A query with a date filter\n",
    "query_analyzer.invoke(\n",
    "    {\"question\": \"videos on chat langchain published in 2023\"}\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4bdf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: A query with a length filter\n",
    "query_analyzer.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\"\n",
    "    }\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa99ac0",
   "metadata": {},
   "source": [
    "<a id='part4'></a>\n",
    "# Advanced Indexing Strategies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b2c26",
   "metadata": {},
   "source": [
    "<a id='part4-1'></a>\n",
    "## Multi-Representation Indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load two different blog posts to create a more diverse knowledge base\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2551c2",
   "metadata": {},
   "source": [
    "Next, weâ€™ll create a chain to generate a summary for each of these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed406fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chain for generating summaries\n",
    "summary_chain = (\n",
    "    # Extract the page_content from the document object\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    # Pipe it into a prompt template\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    # Use an LLM to generate the summary\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\", max_retries=0)\n",
    "    # Parse the output into a string\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Use .batch() to run the summarization in parallel for efficiency\n",
    "summaries = summary_chain.batch(docs, {\"max_concurrency\": 5})\n",
    "\n",
    "# Let's inspect the first summary\n",
    "print(summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to index the summary embeddings\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\" # This key will link summaries to their parent documents\n",
    "\n",
    "# The retriever that orchestrates the whole process\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Generate unique IDs for each of our original documents\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Create new Document objects for the summaries, adding the 'doc_id' to their metadata\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add the summaries to the vectorstore\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\n",
    "# Add the original documents to the docstore, linking them by the same IDs\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3977efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Memory in agents\"\n",
    "\n",
    "# First, let's see what the vectorstore finds by searching the summaries\n",
    "sub_docs = vectorstore.similarity_search(query, k=1)\n",
    "print(\"--- Result from searching summaries ---\")\n",
    "print(sub_docs[0].page_content)\n",
    "print(\"\\n--- Metadata showing the link to the parent document ---\")\n",
    "print(sub_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the full retriever do its job\n",
    "retrieved_docs = retriever.invoke(query, n_results=1)\n",
    "\n",
    "# Print the beginning of the retrieved full document\n",
    "print(\"\\n--- The full document retrieved by the MultiVectorRetriever ---\")\n",
    "print(retrieved_docs[0].page_content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c46b4",
   "metadata": {},
   "source": [
    "<a id='part4-2'></a>\n",
    "## Hierarchical Indexing (RAPTOR) Knowledge Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3ef68",
   "metadata": {},
   "source": [
    "<a id='part4-3'></a>\n",
    "## Token-Level Precision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3db65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Wikipedia content\n",
    "def get_wikipedia_page(title: str) -> str:\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"MyFastEmbedApp/1.0\"}\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page.get(\"extract\", \"\")\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n",
    "\n",
    "# Split the text into smaller chunks (token-level approximation)\n",
    "# Use small chunk size (e.g., 50-100 characters) to mimic token-level embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "chunks = text_splitter.split_text(full_document)\n",
    "\n",
    "# Convert chunks to Document objects\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Create Chroma vector store with embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=OpenAIEmbeddings(),  # or any other embedding model\n",
    "    collection_name=\"Miyazaki-FastEmbed-TokenLevel\"\n",
    ")\n",
    "\n",
    "# Perform similarity search\n",
    "query = \"What animation studio did Miyazaki found?\"\n",
    "results = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "# Print results\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Result {i}: {doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e02607",
   "metadata": {},
   "source": [
    "<a id='part5'></a>\n",
    "# Advanced Retrieval & Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07893be",
   "metadata": {},
   "source": [
    "<a id='part5-1'></a>\n",
    "## Dedicated Re-ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to set your COHERE_API_KEY environment variable\n",
    "# os.environ['COHERE_API_KEY'] = '<your-cohere-api-key>'\n",
    "\n",
    "# Load, split, and index the document\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",))\n",
    "blog_docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# First-pass retriever: get the top 10 potentially relevant documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Cohere Rerank model\n",
    "compressor = CohereRerank(model=\"rerank-multilingual-v3.0\")\n",
    "\n",
    "# Create the compression retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, \n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# Let's test it with our query\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "\n",
    "# Print the re-ranked documents\n",
    "print(\"--- Re-ranked and Compressed Documents ---\")\n",
    "for doc in compressed_docs:\n",
    "    print(f\"Relevance Score: {doc.metadata['relevance_score']:.4f}\")\n",
    "    print(f\"Content: {doc.page_content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad5330",
   "metadata": {},
   "source": [
    "<a id='part6'></a>\n",
    "# Manual RAG Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954fafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a powerful LLM like gpt-4o to act as our \"judge\" for reliable evaluation.\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "\n",
    "# Define the output schema for our evaluation score to ensure consistent, structured output.\n",
    "class ResultScore(BaseModel):\n",
    "    score: float = Field(..., description=\"The score of the result, ranging from 0 to 1 where 1 is the best possible score.\")\n",
    "\n",
    "# This prompt template clearly instructs the LLM on how to score the answer's correctness.\n",
    "correctness_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"ground_truth\", \"generated_answer\"],\n",
    "    template=\"\"\"\n",
    "    Question: {question}\n",
    "    Ground Truth: {ground_truth}\n",
    "    Generated Answer: {generated_answer}\n",
    "\n",
    "    Evaluate the correctness of the generated answer compared to the ground truth.\n",
    "    Score from 0 to 1, where 1 is perfectly correct and 0 is completely incorrect.\n",
    "    \n",
    "    Score:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# We build the evaluation chain by piping the prompt to the LLM with structured output.\n",
    "correctness_chain = correctness_prompt | llm.with_structured_output(ResultScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5130ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_correctness(question, ground_truth, generated_answer):\n",
    "    \"\"\"A helper function to run our custom correctness evaluation chain.\"\"\"\n",
    "    result = correctness_chain.invoke({\n",
    "        \"question\": question, \n",
    "        \"ground_truth\": ground_truth, \n",
    "        \"generated_answer\": generated_answer\n",
    "    })\n",
    "    return result.score\n",
    "\n",
    "# Test the correctness chain with a partially correct answer.\n",
    "question = \"What is the capital of France and Spain?\"\n",
    "ground_truth = \"Paris and Madrid\"\n",
    "generated_answer = \"Paris\"\n",
    "score = evaluate_correctness(question, ground_truth, generated_answer)\n",
    "\n",
    "print(f\"Correctness Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8539b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt template for faithfulness includes several examples (few-shot prompting)\n",
    "# to make the instructions to the judge LLM crystal clear.\n",
    "faithfulness_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\",\"context\", \"generated_answer\"],\n",
    "    template=\"\"\"\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Generated Answer: {generated_answer}\n",
    "\n",
    "    Evaluate if the generated answer to the question can be deduced from the context.\n",
    "    Score of 0 or 1, where 1 is perfectly faithful *AND CAN BE DERIVED FROM THE CONTEXT* and 0 otherwise.\n",
    "    You don't mind if the answer is correct; all you care about is if the answer can be deduced from the context.\n",
    "    \n",
    "    Example:\n",
    "    Question: What is the capital of France and Spain?\n",
    "    Context: Paris is the capital of France and Madrid is the capital of Spain.\n",
    "    Generated Answer: Paris\n",
    "    in this case the generated answer is faithful to the context so the score should be *1*.\n",
    "    \n",
    "    Example:\n",
    "    Question: What is 2+2?\n",
    "    Context: 4.\n",
    "    Generated Answer: 4.\n",
    "    In this case, the context states '4', but it does not provide information to deduce the answer to 'What is 2+2?', so the score should be 0.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Build the faithfulness chain using the same structured LLM.\n",
    "faithfulness_chain = faithfulness_prompt | llm.with_structured_output(ResultScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(question, context, generated_answer):\n",
    "    \"\"\"A helper function to run our custom faithfulness evaluation chain.\"\"\"\n",
    "    result = faithfulness_chain.invoke({\n",
    "        \"question\": question, \n",
    "        \"context\": context, \n",
    "        \"generated_answer\": generated_answer\n",
    "    })\n",
    "    return result.score\n",
    "\n",
    "# Test the faithfulness chain. The answer is correct, but is it faithful?\n",
    "question = \"what is 3+3?\"\n",
    "context = \"6\"\n",
    "generated_answer = \"6\"\n",
    "score = evaluate_faithfulness(question, context, generated_answer)\n",
    "\n",
    "print(f\"Faithfulness Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6c404",
   "metadata": {},
   "source": [
    "<a id='part7'></a>\n",
    "# Evaluation with Frameworks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf89250",
   "metadata": {},
   "source": [
    "<a id='part7-1'></a>\n",
    "## Rapid Evaluation with `deepeval`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed433dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases\n",
    "test_case_correctness = LLMTestCase(\n",
    "    input=\"What is the capital of Spain?\",\n",
    "    expected_output=\"Madrid is the capital of Spain.\",\n",
    "    actual_output=\"MadriD.\"\n",
    ")\n",
    "\n",
    "test_case_faithfulness = LLMTestCase(\n",
    "    input=\"How much is 3+3?\",\n",
    "    expected_output=\"6\",\n",
    "    actual_output=\"6\"\n",
    ")\n",
    "\n",
    "coherence_metric = GEval(\n",
    "    name=\"Match\",\n",
    "    criteria=\"Check if the model output matches the correct answer exactly.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")\n",
    "\n",
    "coherence_metric.measure(test_case_correctness)\n",
    "print(coherence_metric.score)\n",
    "print(coherence_metric.reason)\n",
    "\n",
    "coherence_metric.measure(test_case_faithfulness)\n",
    "print(coherence_metric.score)\n",
    "print(coherence_metric.reason)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be6a07",
   "metadata": {},
   "source": [
    "<a id='part7-2'></a>\n",
    "## Another Powerful Alternative with `grouse`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = GroundedQAEvaluator()\n",
    "unfaithful_sample = EvaluationSample(\n",
    "    input=\"Where is the Eiffel Tower located?\",\n",
    "    actual_output=\"The Eiffel Tower is located in Marseille.\",\n",
    "    expected_output=\"The Eiffel Tower is located at Rue Rabelais in Paris.\",\n",
    "    references=[\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France\",\n",
    "        \"Gustave Eiffel died in his appartment at Rue Rabelais in Paris.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(eval_samples=[unfaithful_sample]).evaluations[0]\n",
    "print(f\"Grouse Faithfulness Score (0 or 1): {result.faithfulness.faithfulness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f03313",
   "metadata": {},
   "source": [
    "<a id='part7-3'></a>\n",
    "## Evaluation with `RAGAS`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare the evaluation data\n",
    "questions = [\n",
    "    \"What is the name of the three-headed dog guarding the Sorcerer's Stone?\",\n",
    "    \"Who gave Harry Potter his first broomstick?\",\n",
    "    \"Which house did the Sorting Hat initially consider for Harry?\",\n",
    "]\n",
    "\n",
    "# These would be the answers generated by our RAG pipeline\n",
    "generated_answers = [\n",
    "    \"The three-headed dog is named Fluffy.\",\n",
    "    \"Professor McGonagall gave Harry his first broomstick, a Nimbus 2000.\",\n",
    "    \"The Sorting Hat strongly considered putting Harry in Slytherin.\",\n",
    "]\n",
    "\n",
    "# The ground truth, or \"perfect\" answers\n",
    "ground_truth_answers = [\n",
    "    \"Fluffy\",\n",
    "    \"Professor McGonagall\",\n",
    "    \"Slytherin\",\n",
    "]\n",
    "\n",
    "# The context retrieved by our RAG system for each question\n",
    "retrieved_documents = [\n",
    "    [\"A massive, three-headed dog was guarding a trapdoor. Hagrid mentioned its name was Fluffy.\"],\n",
    "    [\"First years are not allowed brooms, but Professor McGonagall, head of Gryffindor, made an exception for Harry.\"],\n",
    "    [\"The Sorting Hat muttered in Harry's ear, 'You could be great, you know, it's all here in your head, and Slytherin will help you on the way to greatness...'\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Structure the data into a Hugging Face Dataset object\n",
    "data_samples = {\n",
    "    'question': questions,\n",
    "    'answer': generated_answers,\n",
    "    'contexts': retrieved_documents,\n",
    "    'ground_truth': ground_truth_answers\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the metrics we want to use for evaluation\n",
    "metrics = [\n",
    "    faithfulness,       # How factually consistent is the answer with the context? (Prevents hallucination)\n",
    "    answer_relevancy,   # How relevant is the answer to the question?\n",
    "    context_recall,     # Did we retrieve all the necessary context to answer the question?\n",
    "    answer_correctness, # How accurate is the answer compared to the ground truth?\n",
    "]\n",
    "\n",
    "# 4. Run the evaluation\n",
    "result = evaluate(\n",
    "    dataset=dataset, \n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# 5. Display the results in a clean table format\n",
    "results_df = result.to_pandas()\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ragvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
